<!DOCTYPE html>
<html lang="">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> FPGA hardware linear regression implementation using fixed-point arithmetic </title>
  <meta name="description" content="Aspiring computer engineering researcher with an interest in Hardware Acceleration and Computer Architecture.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="FPGA hardware linear regression implementation using fixed-point arithmetic" />
<meta property="og:description" content="ABSTRACT In this paper, a hardware design based on the field programmable gate array (FPGA) to implement a linear regression algorithm is presented. The arithmetic operations were optimized by applying a fixed-point number representation for all hardware based computations. A floating-point number training data point was initially created and stored in a personal computer (PC) which was then converted to fixed-point representation and transmitted to the FPGA via a serial communication link." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://anthonyabeo.github.io/projects/linear_regression_accelerator/" />
<meta property="article:published_time" content="2020-09-25T10:25:33+00:00" />
<meta property="article:modified_time" content="2020-09-25T10:25:33+00:00" />

  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="FPGA hardware linear regression implementation using fixed-point arithmetic"/>
<meta name="twitter:description" content="ABSTRACT In this paper, a hardware design based on the field programmable gate array (FPGA) to implement a linear regression algorithm is presented. The arithmetic operations were optimized by applying a fixed-point number representation for all hardware based computations. A floating-point number training data point was initially created and stored in a personal computer (PC) which was then converted to fixed-point representation and transmitted to the FPGA via a serial communication link."/>

  
  
    
  
  
  <link rel="stylesheet" href="https://anthonyabeo.github.io/css/style-dark.css">
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://anthonyabeo.github.io/images/favicon.ico" />

  
</head>
<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

    <header id="header">
  <a href="https://anthonyabeo.github.io/">
  
    <div id="logo" style="background-image: url(https://anthonyabeo.github.io/images/logo.png)"></div>
  
  <div id="title">
    <h1>Anabila</h1>
  </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#"><i class="fas fa-bars fa-2x"></i></a>
      </li>
       
        <li><a href="/">Home</a></li>
       
        <li><a href="/posts">All posts</a></li>
       
        <li><a href="/projects">Projects</a></li>
       
        <li><a href="/about">About</a></li>
      
    </ul>
  </div>
</header>
  

    
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <div class="content" itemprop="articleBody">
  
    <h2 id="abstract">ABSTRACT</h2>
<p>In this paper, a hardware design based on the field programmable gate array (FPGA) to implement a linear regression algorithm is presented. The arithmetic operations were optimized by applying a fixed-point number representation for all hardware based computations. A floating-point number training data point was initially created and stored in a personal computer (PC) which was then converted to fixed-point representation and transmitted to the FPGA via a serial communication link. With the proposed VHDL design description synthesized and implemented within the FPGA, the custom hardware architecture performs the linear regression algorithm based on matrix algebra considering a fixed size training data point set. To validate the hardware fixed-point arithmetic operations, the same algorithm was implemented in the Python language and the results of the two computation approaches were compared. The power consumption of the proposed embedded FPGA system was estimated to be 136.82 mW.</p>
<h2 id="discussion">DISCUSSION</h2>
<h3 id="introduction">INTRODUCTION</h3>
<p>As consumers continue to demand more performance, lower power consumption, and cost from technology, engineers have to develop innovative ways to achieve that. One of such methods is to accelerate essential, computationally intensive, power-hungry applications in hardware. One such application area is machine learning. The ubiquity and utility of such an application domain have necessitated a barrage of research in the area of machine learning acceleration.  The authors of this paper are no exception, and in this paper, they provide an example of a potential machine learning acceleration in hardware using an FPGA.</p>
<h3 id="summary">SUMMARY</h3>
<p>This paper&rsquo;s core objective is to demonstrate an implementation of a simple linear regression algorithm in hardware. The algorithm is then trained on a small sample of eight univariate training data points to generate the regression equation&rsquo;s coefficient and intercept. The authors must first convert the training data from floating to fixed-point representation before being transmitted to the FPGA via serial communication. The authors do an excellent job of making their methodology as straightforward as possible using diagrams where necessary. The authors also clearly define technical concepts, but a more in-depth explanation and overall writing quality are lacking, obfuscating the message in some parts of the paper.</p>
<h3 id="implementation-and-results">IMPLEMENTATION AND RESULTS</h3>
<p>I implemented the proposed solution in SystemVerilog and simulated it initially using the authors&rsquo; data points and achieve the same coefficient and intercept result. Furthermore, I tested the implementation of a more massive training data set of 500 training data, which yielded the same coefficient and intercept of the equation when I simulated the same data points using scikit-learn&rsquo;s LinearRegression library.</p>
<h3 id="shortcomings-and-potential-improvements">SHORTCOMINGS AND POTENTIAL IMPROVEMENTS</h3>
<ul>
<li>The author preferred spatial parallelism to temporal parallelism. Using a small dataset ensured that they could parallelize the hardware to execute in one clock cycle.
Their implementation consumed 89% of the FPGA&rsquo;s embedded multipliers and did not use any on-chip memory.</li>
<li>Clearly, there is not enough room for unlimited spatial parallelism on an FPGA, which becomes a bottleneck for more massive datasets.</li>
<li>to support training using large amounts of data and perhaps implementing more complex machine learning algorithms, an FSM based implementation of such algorithms that utilizes some memory and a smaller hardware real estate will be essential.</li>
</ul>
<h3 id="conclusion">CONCLUSION</h3>
<p>This project&rsquo;s goal was to correctly implement a linear regression algorithm on an FPGA, albeit to process a tiny amount of training data. From the author&rsquo;s implementation, achieving this came at the expense of increasing hardware real estate usage, which all rendered this methodology inefficient for more complex algorithms and substantial training data sets.
As noted by the authors, a neural network implementation will require a more innovative approach.</p>
<p>Code is located here: <a href="https://github.com/anthonyabeo/linear_regression_accelerator">https://github.com/anthonyabeo/linear_regression_accelerator</a>.</p>
<h2 id="references">REFERENCES</h2>
<p>Willian de Assis Pedrobon Ferreira, Ian Grout, and Alexandre César Rodrigues da Silva. 2019. FPGA hardware linear regression implementation using fixed-point arithmetic. In <!-- raw HTML omitted -->Proceedings of the 32nd Symposium on Integrated Circuits and Systems Design<!-- raw HTML omitted --> (<!-- raw HTML omitted -->SBCCI &lsquo;19<!-- raw HTML omitted -->). Association for Computing Machinery, New York, NY, USA, Article 10, 1–6. DOI:https://doi.org/10.1145/3338852.3339853</p>

  
  </div>
</article>


    <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2020  Anabila 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts">All posts</a></li>
         
        <li><a href="/projects">Projects</a></li>
         
        <li><a href="/about">About</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
  
</body>

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/js/main.js"></script>
</html>
